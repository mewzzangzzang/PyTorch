{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1ede3f201b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "# https://wikidocs.net/60572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=torch.FloatTensor([1,2,3])\n",
    "# h=F.softmax(z,dim=0)\n",
    "# print(h)\n",
    "# print(h.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=torch.rand(3,5,requires_grad=True)\n",
    "h=F.softmax(z,dim=1)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "y=torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "# 3행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot=torch.zeros_like(h)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [2],\n",
      "        [1]])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print(y.unsqueeze(1))\n",
    "y_one_hot.scatter_(1,y.unsqueeze(1),1) #dim, index, source\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost=(y_one_hot* -torch.log(h)).sum(dim=1).mean()\n",
    "print(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(F.softmax(z,dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
       "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
       "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
       "       grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log_softmax(z,dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = (y_one_hot * -torch.log(h)).sum(dim=1).mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_one_hot * -F.log_softmax(z, dim=1)).sum(dim=1).mean()\n",
    "# cost = (y_one_hot * -torch.log(h)).sum(dim=1).mean() 와 결과 같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 손실함수를 적을때 간단한 방법\n",
    "F.nll_loss(F.log_softmax(z,dim=1),y)\n",
    "# nll-> Negative log Likelihood 약자로 \n",
    "# 원-핫 벡터를 넣을 필요없이 바로 실제값을 인자로 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F.cross_entropy()는 F.log_softmax()와 F.nll_loss()를 포함하고 있습니다.\n",
    "# F.cross_entropy는 비용 함수에 소프트맥스 함수까지 \n",
    "# 포함하고 있음을 기억하고 있어야 구현 시 혼동하지 않습니다.\n",
    "F.cross_entropy(z,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_train=[[1,2,1,1],[2,1,3,2],[3,1,3,4],[4,1,5,5],[1,7,5,5],[1,2,5,6],[1,6,6,6],[1,7,7,7]]\n",
    "y_train=[2,2,2,1,1,1,0,0]\n",
    "x_train=torch.FloatTensor(x_train)\n",
    "y_train=torch.LongTensor(y_train)\n",
    "print(x_train.shape)\n",
    "print(y_train.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot=torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost:1.0986123085021973\n",
      "epoch: 100, cost:0.7041996121406555\n",
      "epoch: 200, cost:0.6229995489120483\n",
      "epoch: 300, cost:0.5657169222831726\n",
      "epoch: 400, cost:0.5152913331985474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 500, cost:0.467661589384079\n",
      "epoch: 600, cost:0.4212779700756073\n",
      "epoch: 700, cost:0.37540146708488464\n",
      "epoch: 800, cost:0.3297656178474426\n",
      "epoch: 900, cost:0.2850724160671234\n",
      "epoch: 1000, cost:0.24815461039543152\n"
     ]
    }
   ],
   "source": [
    "w=torch.zeros((4,3), requires_grad=True)\n",
    "b=torch.zeros((1,3), requires_grad=True)\n",
    "# 회귀식이 1행 3열\n",
    "optimizer=optim.SGD([w,b], lr=0.1)\n",
    "for epoch in range(1001):\n",
    "  # 식이 3가지 이므로 dim=1\n",
    "  hat=F.softmax(x_train.matmul(w)+b, dim=1)\n",
    "  cost=(y_one_hot * -torch.log(hat)).sum(dim=1).mean()\n",
    "\n",
    "  optimizer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  if epoch % 100 ==0:\n",
    "    print('epoch: {}, cost:{}'.format(epoch,cost.item()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1909e-04, 7.5725e-03, 9.9231e-01],\n",
      "        [3.9348e-03, 1.3411e-01, 8.6196e-01],\n",
      "        [6.9057e-06, 2.6787e-01, 7.3212e-01],\n",
      "        [3.1690e-05, 7.8606e-01, 2.1390e-01],\n",
      "        [3.2070e-01, 6.5953e-01, 1.9770e-02],\n",
      "        [1.6596e-01, 8.3388e-01, 1.6129e-04],\n",
      "        [6.3023e-01, 3.6949e-01, 2.7694e-04],\n",
      "        [8.0781e-01, 1.9218e-01, 1.4840e-05]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=F.softmax(x_train.matmul(w)+b,dim=1)\n",
    "print(pred)\n",
    "pred.argmax(dim=1)\n",
    "# pred.argmax - 제일 큰값의 인덱스 산출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost:1.845719814300537\n",
      "epoch: 100, cost:0.647149920463562\n",
      "epoch: 200, cost:0.5688682794570923\n",
      "epoch: 300, cost:0.5156992077827454\n",
      "epoch: 400, cost:0.4717271625995636\n",
      "epoch: 500, cost:0.4324863851070404\n",
      "epoch: 600, cost:0.39587944746017456\n",
      "epoch: 700, cost:0.36050671339035034\n",
      "epoch: 800, cost:0.32522761821746826\n",
      "epoch: 900, cost:0.2892170250415802\n",
      "epoch: 1000, cost:0.2540857791900635\n"
     ]
    }
   ],
   "source": [
    "model=nn.Linear(4,3)\n",
    "optimzer=optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "for epoch in range(1001):\n",
    "  # 식이 3가지 이므로 dim=1\n",
    "  y_hat=model(x_train)\n",
    "  cost=F.cross_entropy(y_hat,y_train)\n",
    "\n",
    "  optimzer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimzer.step()\n",
    "\n",
    "  if epoch % 100 ==0:\n",
    "    print('epoch: {}, cost:{}'.format(epoch,cost.item()))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassModel(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.linear=nn.Linear(4,3)\n",
    "\n",
    "  def forward(self,x):\n",
    "    return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, cost:1.9578617811203003\n",
      "epoch: 100, cost:0.6483842730522156\n",
      "epoch: 200, cost:0.566425085067749\n",
      "epoch: 300, cost:0.5118246078491211\n",
      "epoch: 400, cost:0.4670933783054352\n",
      "epoch: 500, cost:0.4272816777229309\n",
      "epoch: 600, cost:0.39008742570877075\n",
      "epoch: 700, cost:0.35401394963264465\n",
      "epoch: 800, cost:0.31791287660598755\n",
      "epoch: 900, cost:0.28127554059028625\n",
      "epoch: 1000, cost:0.24926359951496124\n"
     ]
    }
   ],
   "source": [
    "model=SoftmaxClassModel()\n",
    "optimzer=optim.SGD(model.parameters(),lr=0.1)\n",
    "  \n",
    "for epoch in range(1001):\n",
    "  y_hat=model(x_train)\n",
    "  cost=F.cross_entropy(y_hat,y_train)\n",
    "  \n",
    "  optimzer.zero_grad()\n",
    "  cost.backward()\n",
    "  optimzer.step()\n",
    "  \n",
    "  if epoch%100 == 0:\n",
    "    print('epoch: {}, cost:{}'.format(epoch,cost.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 1, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_train).argmax(dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
